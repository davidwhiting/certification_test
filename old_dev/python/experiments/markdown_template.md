# Driverless AI Experiment: `{{ experiment.description }}`

Generated on: {{ generated_on }}

Generated by: {{ generated_by }}

## Table of Contents

1. [Experiment Overview](#experiment-overview)
2. [Data Overview](#data-overview)
3. [Methodology](#methodology)
4. [Data Sampling](#data-sampling)
5. [Validation Strategy](#validation-strategy)
6. [Model Tuning](#model-tuning)
7. [Feature Evolution](#feature-evolution)
8. [Feature Transformation](#feature-transformation)
9. [Final Model](#final-model)
10. [Alternative Models](#alternative-models)
11. [Deployment](#deployment)
12. [Partial Dependence Plots](#pdp){% if mli and experiment_overview._problem_type != "multinomial" %}
13. [Global K-LIME GLM Coefficients] (#klime){% endif %}
14. [Appendix](#appendix)

## Experiment Overview <a name="experiment-overview"></a>

Driverless AI built {% if final_model._final_model_type == "stacked ensemble" -%}
a stacked ensemble of {{final_model._final_model_string}}{% elif final_model._final_model_type == "pasting ensemble" %}a bagged ensemble (pasting{% set reference_list = True %}{% set pasting_reference = True %}{% set pasting_ref_num = 1%}[{{pasting_ref_num}}]) of {{final_model._final_model_string}} across {{final_model._num_folds}} folds{% else -%} {{final_model._final_model_string}}{%- endif %} to predict`{{params._params.target_col}}` given {{"{:,}".format(final_features._original_features | length)}} original features from the input dataset `{{train_data.name}}`. This{% if params._params.is_classification %} classification {% else %} regression {% endif %}experiment completed in {{experiment_overview._experiment_duration.formatted_time}} ({{experiment_overview._experiment_duration.string_time}}), using {{"{:,}".format(final_features.get_num_original_features_used())}} of the {{"{:,}".format(final_features._original_features | length)}} original features, and {{"{:,}".format(final_features.get_num_transformed_features_used())}} of the {{"{:,}".format(final_features.get_num_transformed_features_survived())}} engineered features. 

### Performance 

{{experiment_overview.get_performance_table()}}



### Driverless Settings

|    Dial Settings   | Description | Setting Value | Range of Possible Values |
|:------------------:|-------------|---------------|--------------------------|
|   Accuracy   | Controls accuracy needs of the model | {{params._params.accuracy}}  | {{accuracy_range}}  |
|      Time      | Controls duration of the experiment | {{params._params.time}} | {{time_range}}     |
| Interpretability | Controls complexity of the model | {{params._params.interpretability}}   | {{interpretability_range}}  |

### System Specifications

{{experiment_overview.get_system_stats_table()}}


### Versions

{{version_info}}

## Data Overview <a name="data-overview"></a>

This section provides information on the datasets used for the experiment.  

{{data_info.get_data_comparison()}}


### Training Data

The training data consists of 
{% if data_info._train_summary.numeric_summary|length > 0 and  data_info._train_summary.categorical_summary|length> 0 -%}
both numeric and categorical columns.  
{% elif data_info._train_summary.categorical_summary|length == 0 -%}
only numeric columns.
{% else -%}
only categorical columns.  
{%- endif %}

The summary of the columns is shown below: 

{% if data_info._train_summary.numeric_summary|length > 0 -%}

#### Numeric Columns

{{data_info._train_summary.numeric_summary}}

{%- endif %}

{% if data_info._train_summary.boolean_summary|length > 0 -%}

#### Boolean Columns

{{data_info._train_summary.boolean_summary}}

{%- endif %}


{% if data_info._train_summary.categorical_summary|length > 0 -%}

#### Categorical Columns

{{data_info._train_summary.categorical_summary}}

{%- endif %}


### Shifts Detected

Driverless AI can perform shift detection between the training, validation, and testing datasets. It does this by training a binomial model to predict which dataset a record belongs to. For example, it may find that it is able to separate the training and testing data with an AUC of 0.8 using only the column: `C1` as the predictor. This indicates that there is some sort of drift in the distribution of `C1` between the training and testing data.

{% if experiment.test_predictions_path == '' and experiment.valid_predictions_path == '' -%}For this experiment, Driverless AI was not able to check for distribution shifts because only the training dataset was supplied by the user.
{% elif experiment_overview._internal_args.check_distribution_shift == False -%}For this experiment, Driverless AI was not able to check for distribution shifts between the {{data_info._comparison_string}} 
 because the accuracy setting was too low. To turn on shift detection, increase the accuracy setting.
{% elif data_info._shift_table == None -%}For this experiment, Driverless AI checked the {{data_info._comparison_string}} for any shift in distributions but found none. This indicates that all the predictors/columns in the {{data_info._comparison_string}} are from the same distribution. 
{% else -%}For this experiment, Driverless AI checked the {{data_info._comparison_string}} for any shift in distribution and found the following significant differences:
{% for shift in data_info._shift_info %}

* <td>{{shift.shift_warning}}</td>

{% if "relative_path" in shift.keys() %}
![shift {{shift.shift_col}}]({{shift.relative_path}}) 
{% endif %}
  
{% endfor %}
{%- endif %}

## Methodology <a name="methodology"></a>

This section describes the experiment methodology.

### Assumptions and Limitations

Driverless AI trains all models based on the training data provided (in this case: `{{experiment.parameters.dataset.display_name}}`). It is the assumption of Driverless AI that this dataset is representative of the data that will be seen when scoring.

Driverless AI may perform shift detection between the {{data_info._comparison_string}}.  If a shift in distribution is detected, this may indicate that the data that will be used for scoring may have distributions not represented in the training data. {%- if valid_predictions_path == '' %}The model may have poorer performance than seen on the internal validation data. {%- endif %}

 For this experiment, Driverless AI
{% if experiment.test_predictions_path == '' and experiment.valid_predictions_path == ''-%}was not able to detect any shift in distribution between {{data_info._comparison_string}} because no validation or test data was provided.
{% elif experiment_overview._internal_args.check_distribution_shift == False -%}did not detect any shift in distribution because shift detection was turned off due to the accuracy setting (increase accuracy setting for shift detection). 
{% elif data_info._shift_table == None -%}performed shift detection but found no significant changes in the distribution of the {{data_info._comparison_string}}. 
{% else -%}performed shift detection and found significant differences described below: 

{{data_info._shift_table}}

{%- endif %}


### Experiment Pipeline

For this experiment, Driverless AI performed the following steps to find the optimal final model: 

![dai pipeline](./images/dai_pipeline.png "DAI Pipeline") 

The steps in this pipeline are described in more detail below:

1. **Ingest Data**
    {% if data_info._sampling_desc.downsampled -%}
    * data filtered from {{"{:,}".format(train_data.row_count)}} to {{"{:,}".format(data_info._sampling_desc.sampled_size)}} rows using {{data_info._sampling_desc.sampling_type}} {%- endif %}
    * detected column types 
2. **Feature Preprocessing**
    *  turned raw features into numeric
3. **Model and Feature Tuning**

	This stage combines random hyperparameter tuning with feature selection and generation. Features in each iteration are updated using variable importance from the previous iteration as a probabilistic prior to decide what new features to create. The best performing model and features are then passed to the feature evolution stage.

    {% if experiment_overview._stage_info.get("Model and feature tuning", {"models": "None" }).models != "None" -%}
    * found the optimal parameters for {{model_tuning._tuning_algos}} by training models with different parameters
    * the best parameters are those that generate the {{scorer_direction}}**{{experiment.score_f_name}}** on the internal validation data
    {% else -%}
    * o	performed no model tuning due to the Accuracy setting (consider increasing Accuracy setting)
    {%- endif %}
    {%-if experiment_overview._stage_info.get("Model and feature tuning", { "models": "None" }).models != "None" %}
    * {{experiment_overview._stage_info.get("Model and feature tuning", {"models": "No"}).models}}  model{% if experiment_overview._stage_info.get("Model and feature tuning", {"models": 0}).models > 1 %}s{% endif %} trained and scored to evaluate features and model parameters {% endif %}

4. **Feature Evolution**

	This stage uses a genetic algorithm to find the best set of model parameters and feature transformations to be used in the final model.
    {% if experiment_overview._stage_info.get("Feature evolution", {"models": "None" }).models == "None" %}
    * trained no models during feature evolution because the user finished the experiment early
    {% else -%}
    * found the best representation of the data for the final model training by creating and evaluating **{{"{:,}"
    .format(final_features.get_num_transformed_features_survived())}}** features over **{{experiment_overview._iteration_info.actual_num_iterations}}** iteration{%- if experiment_overview._iteration_info.actual_num_iterations > 1 %}s {%- endif %}
    * trained and scored {{experiment_overview._stage_info.get("Feature evolution", {"models": ""}).models}}model{%  if experiment_overview._stage_info.get("Feature evolution", {"models": 0 }).models > 1%}s{%  endif %} trained and scored to further evaluate engineered features
    {%- endif %}
5. **Final Model**
    {% if final_model._final_model_type == "single model" %}
    * created the best model from the feature engineering iterations 
    	* no stacked ensemble is done{% if experiment.valid_predictions_path == '' and (params._params.time_col == '[OFF]') %} due to accuracy or ensemble level settings (consider increasing accuracy or the ensemble_level) {% elif  params._params.time_col != '[OFF]' %} because a time column was provided {% elif experiment_overview._valid_data %} because a validation dataset was provided by the user{% else %} because the ensemble level was set to 0 {% endif %} {% elif final_model._final_model_type == "pasting ensemble" %}
    	* the final model is a bagged ensemble (pasting) of **{{final_model._final_model_string}}** across {{final_model._num_folds}} folds.{% else %}
    	* the final model is a stacked ensemble of **{{final_model._final_model_string|e}}**
    	* the features of {%- if final_model._num_ensemble_models > 1 %} these models {%- else %} this model {%- endif %} are the best features found during the feature engineering iterations
    {%- endif %}
6. **Create Scoring Pipeline**
    {% if experiment.mojo_pipeline_path != None -%}
    * created and exported the MOJO and Python scoring pipeline
    	* MOJO Scoring Pipeline: `{{experiment.mojo_pipeline_path}}`
    	* Python Scoring Pipeline: `{{experiment.scoring_pipeline_path}}`
    {% else %}
    * created and exported the Python scoring pipeline (no MOJO Scoring Pipeline automatically created)
    	* Python Scoring Pipeline: `{{experiment.scoring_pipeline_path}}`
    {%- endif %}
    
Driverless AI trained models throughout the experiment in an effort to determine the best parameters, model dataset, and optimal final model. The stages are described below:

| Driverless AI Stage | Timing (seconds) | Number of Models        | 
|--------------------|-------------------------|-------------------------------|
| Data Preparation | {{experiment_overview._stage_info.get('Data preparation', { "time": "Not available for this type of experiment" }).time}} | {{experiment_overview._stage_info.get('Data preparation', { "models": "Not available for this type of experiment" }).models}} | 
| Model and Feature Tuning   |  {{experiment_overview._stage_info.get('Model and feature tuning', { "time": "Not available for this type of experiment" }).time}} | {{experiment_overview._stage_info.get('Model and feature tuning', { "models": "Not available for this type of experiment" }).models}}| 
| Feature Evolution | {{experiment_overview._stage_info.get('Feature evolution', { "time": "Not available for this type of experiment" }).time}} | {{experiment_overview._stage_info.get('Feature evolution', { "models": "Not available for this type of experiment" }).models}} | 
| Final Pipeline Training   | {{experiment_overview._stage_info.get('Final pipeline training', { "time": "Not available for this type of experiment" }).time}} | {{experiment_overview._stage_info.get('Final pipeline training', { "models": "Not available for this type of experiment" }).models}}| 

### Experiment Settings

Below are the settings selected for the experiment by {{generated_by}}. The Defined Parameters represent the high-level parameters.

**Defined Parameters**

{{params.get_user_defined_parameters()}}


These Accuracy, Time, and Interpretability settings map to the following internal configuration of the Driverless AI experiment: 

{{params.get_internal_parameters_table()}}

#### Details

* **data filtered**: Driverless AI may filter the training data depending on the number of rows and the Accuracy setting.
    * for this experiment,{% if experiment_overview._internal_args.max_rows < train_data.row_count -%} the training data was filtered from {{"{:,}".format(train_data.row_count)}} to {{"{:,}".format(experiment_overview._internal_args.max_rows)}} rows. This filtering can be prevented by increasing the Accuracy setting. {% else -%} the training data was not filtered. {%- endif %}
{% if params._params.is_classification == False -%}
* **tune target transform**: whether Driverless AI evaluated the model performance if the target was transformed.
    * ex: the model performance may be better by predicting the log of the target column instead of the raw target column 
{%- endif %}
* **number of feature engineering iterations**: the number of iterations performed of feature engineering.
* **number of models evaluated per iteration**: for each feature engineering iteration, Driverless AI trains multiple models. Each model is trained with a different set of predictors or features. The goal of this step is to determine which types of features lead to the {{scorer_direction}} {{experiment.score_f_name}}.
* **early stopping rounds**: if Driverless AI does not see any improvement after {{experiment_overview._internal_args.early_stopping_rounds}} iterations of feature engineering, the feature engineering step is automatically stopped.
* **monotonicity constraint**: if enabled, the models will only have monotone relationships between the predictors and target variable.
* **number of model tuning combinations**: the number of model tuning combinations evaluated to determine the optimal model settings for the {{feature_evolution._feature_evolution_algos}}.
* **number of base learners in ensemble**: the number of base models used to create the final ensemble.  
* **time column**: the column that provides the time column. If a time column is provided, feature engineering and model validation will respect the causality of time. If the time column is turned off, no time order is used for modeling and data may be shuffled randomly (any potential temporal causality will be ignored).
{% if params._params.time_col != '[OFF]' -%}
* **time group columns**: the columns that make up the time series groups. 
* **time period**: the periodicity found in the dataset.
* **number of prediction periods**: the number of periods you want to predict in advance.
* **number of gap periods**: the gap between the data available and the forecast period desired.  
{%- endif %}

## Data Sampling <a name="data-sampling"></a>

{% if data_info._sampling_desc.downsampled %}
Driverless AI sampled down the data because of the *statistical_threshold_data_size_large* setting and accuracy setting. The data was filtered from {{"{:,}".format(train_data.row_count)}} to {{"{:,}".format(data_info._sampling_desc.sampled_size)}} rows.  
{% if  data_info._sampling_desc.imbalanced %}
Driverless AI performed imbalanced data sampling because the majority class of the target variable occurs more than {{config.imbalance_ratio_sampling_threshold}} times as often as the minority class.   The majority class occurs {{ "{:,}".format( data_info._sampling_desc.majority_count)}} times in the training data and the minority class only occurs {{"{:,}".format( data_info._sampling_desc.minority_count)}} times.
Because the target class distribution was imbalanced, Driverless AI performed {{data_info._sampling_desc.sampling_type}}.
{% else %}
Driverless AI used {{data_info._sampling_desc.sampling_type}} to reduce the size of the training data.
{% endif %}{% else %}
Driverless AI did not perform any down sampling of the data.
{% endif %}

## Validation Strategy <a name="validation-strategy"></a>

{% if params._params.time_col == '[OFF]'%}

{% if experiment.valid_predictions_path == '' %}

Driverless AI automatically split the training data to determine the performance of the model parameter tuning and feature engineering steps.

For the experiment, Driverless AI{% if experiment_overview._internal_args.only_first_fold_model and experiment_overview._internal_args.fold_reps > 1 -%} randomly split the data into **{{experiment_overview._internal_args.num_folds - 1}}/{{ experiment_overview._internal_args.num_folds}} training** and **1/{{experiment_overview._internal_args.num_folds}} validation** {{experiment_overview._internal_args.fold_reps}} times. The split was repeated more than once so that more data could be used for training. The visualization below shows how the data was split into training and validation.

![internal validation]({{validation_schema._plots.internal_validation.relative_path}} "Internal Validation")

{% elif experiment_overview._internal_args.only_first_fold_model and experiment_overview._internal_args.fold_reps <= 1 -%}
randomly split the data into **{{experiment_overview._internal_args.num_folds - 1}}/{{ experiment_overview._internal_args.num_folds}} training** and **1/{{experiment_overview._internal_args.num_folds}} validation**. 

{% elif experiment_overview._internal_args.num_folds != '5' -%}
randomly split the data into {{ experiment_overview._internal_args.num_folds}} fold cross validation. With cross validation, the whole dataset is utilized by training {{ experiment_overview._internal_args.num_folds}} models where each model is trained on a different subset of the training data.

The visualization below shows how cross validation is utilized to get predictions on hold out data. The visualization shows an example of cross validation with 5 folds. For this experiment, however, {{experiment_overview._internal_args.num_folds}} folds were created.

![cross validation](./images/cv_schema.png "Cross Validation") 

{% if experiment_overview._internal_args.fold_reps > 1 -%}
Note: The cross-validation process was repeated {{experiment_overview._internal_args.fold_reps | int}} times to ensure the validation metrics are robust since the training data was small.
{% endif -%}

{% else -%}

randomly split the data into {{ experiment_overview._internal_args.num_folds}} fold cross validation. With cross validation, the whole dataset is utilized by training {{ experiment_overview._internal_args.num_folds}} models where each model is trained on a different subset of the training data.

The visualization below shows how cross validation with 5 folds is utilized to get predictions on hold out data. 

![cross validation](./images/cv_schema.png "Cross Validation") 

{% if experiment_overview._internal_args.fold_reps > 1 -%}
Note: The cross-validation process was repeated {{experiment_overview._internal_args.fold_reps | int}} times to ensure the validation metrics are robust since the training data was small.
{% endif -%}

{%- endif %}

{%- endif %}

{% else %}

{% endif %}

{% if experiment.valid_predictions_path != '' %}

Driverless AI used the validation data provided (`{{experiment_overview._internal_args.valid_data_name}}`) to determine the performance of the model parameter tuning and feature engineering steps.

{% endif %}

{% if params._params.time_col != '[OFF]' and params._config_overrides.time_series_recipe == True -%}

Driverless AI automatically split the data into training and validation data, ordering the data by `{{params._params.time_col}}`. The experiment predicted {{params._params.num_prediction_periods}} {%-if params._params.num_prediction_periods > 1 %} **{{params._params.time_unit}}s** {%- else %} **{{params._params.time_unit}}** {%- endif %} ahead {%- if params._params.num_gap_periods == 0 %} with no gap between training and forecasting. {%- else %} with a {{params._params.num_gap_periods}} **{{params._params.time_unit}}** gap between training and forecasting. {%- endif %}

{% endif %}

## Model Tuning <a name="model-tuning"></a>

{% if model_tuning._tuning_tables | length == 0 -%}
No parameter tuning was done. Consider increasing your accuracy setting.
{% elif model_tuning._tuning_tables | length == 1 -%}
The table below shows a portion of the different parameter configurations evaluated by Driverless AI for the {{model_tuning._tuning_algos}} and their score and training time. The table is ordered based on a combination of {{scorer_direction}} score and lowest training time.

{% for key, value in model_tuning._tuning_tables.items()%}
{{value}}
{% endfor %}

{% else %}

The table below shows the score and training time of the {{model_tuning._tuning_algos}} evaluated by Driverless AI. The table shows the {% if model_tuning._num_tuning_models > 10 %}top 10 {% endif %}parameter tuning models evaluated, ordered based on a combination of {{scorer_direction}} score and lowest training time.

{{model_tuning._tuning_tables.get("combo")}}

More detailed information on the parameters evaluated for each algorithm is shown below.

{% for key, value in model_tuning._tuning_tables.items() if key != "combo"%}

### {{key}} tuning

{{value}}
{% endfor %}

{%- endif %}


## Feature Evolution <a name="feature-evolution"></a>

{% if experiment_overview._stage_info.get("Feature evolution", { "models": "None" }).models != "None" %}

{%if experiment_overview._stage_info.get("Model and feature tuning", { "models": "None" }).models != "None" -%} During the Model and Feature Tuning Stage, Driverless AI evaluates the effects of different types of algorithms, algorithm parameters, and features. The goal of the Model and Feature Tuning Stage is to determine the best algorithm and parameters to use during the Feature Evolution Stage.{% endif -%} In the Feature Evolution Stage, Driverless AI trained {{feature_evolution._feature_evolution_algos}} ({{experiment_overview._stage_info.get("Feature evolution", {"models": "models"}).models}}) where each model evaluated a different set of features. The Feature Evolution Stage uses a genetic algorithm to search the large feature engineering space.  

The graph below shows the effect the{%-if experiment_overview._stage_info.get("Model and feature tuning", {"models": "None"}).models != "None" %} Model and Feature Tuning Stage and{% endif %} Feature Evolution Stage had on the performance.

![iteration_performance_graph]({{alternative_models._plots.iteration_scores.relative_path}} "Iteration Performance Graph") 

{% else %}

The goal of the Feature Evolution stage is to determine the best features to use for the final model.  Feature Evolution was not performed during this experiment because the user finished the experiment early.

{% endif %}

Based on the experiment settings and column types in the dataset, Driverless AI was able to explore the following transformers: 

{% for key, value in feature_evolution._feature_transformers_available.items() %}
* **{{key}}**: {{value}} 
{% endfor %}


## Feature Transformation <a name="feature-transformation"></a>

The result of the Feature Evolution Stage is a set of features to use for the final model.{%- if final_features.get_num_transformed_features_used() > 0 %} Some of these features were automatically created by Driverless AI. {% endif -%}

{% if config.autodoc_num_features != None or config.autodoc_min_relative_importance != None %}The top features used in the final model are shown below, ordered by importance.{% if config.autodoc_num_features != None and config.autodoc_min_relative_importance != None %} The features in the table{% if config.autodoc_num_features == -1%} include all features{% else %} are limited to the top {{config.autodoc_num_features}}{% endif %}, restricted to those with relative importance greater than or equal to {{config.autodoc_min_relative_importance}}. 
{% elif config.autodoc_num_features != None and config.autodoc_min_relative_importance == None %}The features in the table{% if config.autodoc_num_features == -1%} include all features{% else %} are limited to the top {{config.autodoc_num_features}}{% endif %}.   
{% elif config.autodoc_num_features == None and config.autodoc_min_relative_importance != None %} 
The features in the table are limited to those with relative importance greater than or equal to {{config.autodoc_min_relative_importance}}.{% endif %}{% endif %}{% if config.autodoc_num_features == None and config.autodoc_min_relative_importance == None %} All {{ final_features._feature_importance | length | int }} features used in the final model are shown below ordered by importance. {% endif %}If no transformer was applied, the feature is an original column.  

{{final_features.get_feature_importance_formatted()}}

{% if config.autodoc_include_permutation_feature_importance %}
{% if config.autodoc_feature_importance_scorer %}
**Permutation based feature importance computed using {{feature_importance._score_f_name}} scorer.**

Permutation-based feature importance shows how much a model's performance would change if a feature's values were permuted. If the feature has little predictive power, shuffling its values should have little impact on the model's performance. If a feature is highly predictive, however, shuffling its values should decrease the model's performance. The difference, between the model's performance before and after permuting the feature, provides the feature's absolute permutation importance. For this permutation importance calculation, each feature was shuffled {{config. autodoc_feature_importance_num_perm}} time{% if config. autodoc_feature_importance_num_perm == 1%}.{% else %}s.{% endif %}{% else %}

**Permutation based feature importance computed using default model scorer.**

Permutation-based feature importance shows how much a model's performance would change if a feature's values were permuted. If the feature has little predictive power, shuffling its values should have little impact on the model's performance. If a feature is highly predictive, however, shuffling its values should decrease the model's performance. The difference, between the model's performance before and after permuting the feature, provides the feature's absolute permutation importance. For this permutation importance calculation, each feature was shuffled {{config. autodoc_feature_importance_num_perm}} time{% if config. autodoc_feature_importance_num_perm == 1%}.{% else %}s.{% endif %}{% endif %}

{{feature_importance.get_feature_importance_table ()}}

**Relative Feature Importance Plot**

![]({{feature_importance.get_plot(20)}})


{% endif %}

## Final Model <a name="final-model"></a>

**Pipeline**

{{final_model._ensemble_json.get("ensemble.txt")}}

{% if final_model.pipeline_plot_flag %}![pipeline](./images/pipeline.png "Pipeline"){% endif %}

**Details**

* The fitted features of the final model are the best features found during the feature engineering iterations. 
* The target transformer indicates the type of transformation applied to the target column. 


{{final_model._final_model_desc.overview}}

{% for key, value in final_model._final_model_desc.base_learners.items()%}

* {{value.string}}

{{value.params}}

{% endfor %}

For a complete list of the parameters of the final model, see the Appendix.

**Performance of Final Model**

{{final_model.get_final_model_scores()}}

{% if params._params.is_classification and final_model._confusion_matrix != {} %}

**Validation Confusion Matrix**

{{final_model._confusion_matrix.validation}}
{%endif %}

{% if params._params.is_classification and experiment.test_score != None and final_model._confusion_matrix != {} %}

**Test Confusion Matrix**

{{final_model._confusion_matrix.test}}
{% endif %}


{% if experiment.test_score == None -%}

{% for key, value in final_model._plots.items() %}

***{{value.desc}}***

![]({{value.validation_relative_path}}) 
  
{% endfor %}

{% else -%}

{% for key, value in final_model._plots.items() %}

***{{value.desc}}***

![]({{value.validation_relative_path}}) ![]({{value.test_relative_path}})
  
{% endfor %}

{% endif %}

{% if final_model._glm_in_final %}**GLM Model Coefficients Table**

The following section includes the fitted standardized coefficients for the Final Model's GLM(s). The Mean, Variance, and Scales (the standardized value) columns correspond to each feature, and can be used to standardize the dataset.

{% if final_model._glm_coef_table[1] %}

{% if (final_model._problem_type == 'multinomial') and (final_model._num_classes > 3) %}

{% for fold_key in final_model._glm_coef_table[0] %}

{{ final_model._glm_coef_table[0][fold_key][1] }}
{{ final_model._glm_coef_table[0][fold_key][2] }}

{% for multic_key in final_model._glm_coef_table[0][fold_key][0] %}

{{final_model._glm_coef_table[0][fold_key][0][multic_key]}}

{% endfor %}

{% endfor %}

{% else %}{% for key in final_model._glm_coef_table[0] %}
{{ final_model._glm_coef_table[0][key][1] }}
{{ final_model._glm_coef_table[0][key][2] }}

{{final_model._glm_coef_table[0][key][0]}}

{% endfor %}{% endif %}{% else %}{% if  (final_model._problem_type == 'multinomial') and (final_model._num_classes > 3) %}{% for key in final_model._glm_coef_table[0] %}

{{final_model._glm_coef_table[0][key]}}

{% endfor %}{% else %}

{{final_model._glm_coef_table[0]}}

{% endif %}

{% endif  %}

{% endif %}

{% if final_model._glm_coef_appdx[0] %}

The full coefficients table or tables can be found in the Appendix section.

{% else %}

{% endif %}

{% if psi and (psi._test_preds.keys() | length > 0) %}
**Population Stability Index (PSI)**

Population Stability Index is a statistic used to describe a variable's distribution shift. It can measure the shift between the training dataset's model score distribution and any other given dataset (i.e. validation or test dataset). 

A PSI value lower than 0.10 indicates a small shift in the model predictions, a value between 0.10 and 0.25 indicates a moderate shift, and a value greater than 0.25 indicates a strong shift. Strong shift values can indicate that the model trained on the training dataset might not be suitable for the provided validation or test datasets.

The PSI and calculation table is provided for each dataset below. The corresponding table columns are defined as follows:

* *Quantile: the bin to which the ordered predicted probabilities belong.*  

* *Upper Bound: the upper bound of the corresponding bin.*

* *Test Count: the total number of Test records within the corresponding bin.*

* *Test Fraction (Tst): Test Count divided by the total number of Test records.*

* *Train Count: the total number of Train records within the corresponding bin.*

* *Train Fraction (Trn): Train Count divided by the total number of Train records.*

* *Tst - Trn: the difference between the Test Fraction and the Train Fraction.*

* *ln(Tst / Trn): the natural logarithm of the Test Fraction divided by the Train Fraction.*

* *PSI: the Population Stability Index for each bin - the dataset PSI is the total sum of these PSI values.*

{% set psi_results =
psi.get_psi(config.autodoc_population_stability_index_n_quantiles) %}{% for split, psi_result in psi_results["psi"].items() %}
**{{split}}**

The Population Stability Index is {{ '{0:.4}'.format(psi_result["score"]) }}.

{{psi_result["table"]}}
{% endfor %}

**Summary PSI table**

{{psi_results["summary"]}}

{% endif %}

{% if pred_stats %}
**Prediction statistics**

The following tables and plots show the min, max, mean, and median quantile prediction values for each dataset split. Note: values are rounded to the fourth decimal place. For example, .000025 and .000010 would both appear as 0.0.

{% for split, prediction_stats in pred_stats. get_prediction_stats (config.autodoc_prediction_stats_n_quantiles).items() %}
**{{split}}**

{{prediction_stats["table"]}}
![]({{prediction_stats["plot"]}})
{% endfor %}{% endif %}

{% if response_rates %}
**Quantile Response Rates**

The response rate, for a given quantile, is equal to the number of positive-labeled data points divided by the total number of data points. Quantiles are sorted in decreasing order.

{% set responses = response_rates.get_response_rates(config.autodoc_response_rate_n_quantiles) %}

![]({{responses["response_rate_plot"]}})

**Actual vs Predicted Probabilities**

This plot shows the alignment between the predicted and the actual probabilities. The predicted probabilities are binned into quantiles. For each, bin the average predicted value and the actual response rate (i.e., the number positive-labeled records divided by the total number of records within each bin) is calculated.

![]({{responses["act_vs_pred_prob"]}})

**Actual vs Predicted Log Odds**

This plot shows the alignment between the predicted and the actual probabilities within the log odds space. In this case, the log odds are the log transformation of the probability of a positive record divided by the probability of a negative record.  

![]({{responses["act_vs_pred_odds"]}})

The corresponding table columns are defined as follows:

* *Quantile: the bin to which the ordered predicted probabilities belong.* 

* *bound: the upper bound of the corresponding bin.*

* *{dataset name} cnt: the number of records within the corresponding bin.*

* *{dataset name} sum: the number of positive-labeled records within the corresponding bin.*

* *{dataset name} act: the fraction of positive-labeled records within the corresponding bin.*

* *{dataset name} pred: the mean of the predicted values that fall within the corresponding bin.*

{{responses["table"]}}

{% endif %}

{% if scorers %}
**Additional Prediction Scores**

{{scorers.get_table()}}
{% endif %}

{% if gini_plot %} 
**Gini Plot**

The Gini Plot shows the equality line and the ROC curve for each dataset. The Gini metric for each dataset is provided in the legend.

![]({{gini_plot.plot()}})
{% endif %}


## Alternative Models <a name="alternative-models"></a>

During the experiment, Driverless AI trained {{"{:,}".format(alternative_models._num_alternative_models | int)}} alternative models. The following algorithms were evaluated during the Driverless AI experiment:

{{alternative_models._algo_details.package_details}}

Driverless AI is able to evaluate the algorithms: XGBoost GBM, XGBoost GLM, LightGBM, RuleFit, Tensorflow, and FTRL models. The table below explains why certain algorithms were not selected for the final model, if any.

{{alternative_models._algo_selection}}


## Deployment <a name="deployment"></a>

{% if experiment.scoring_pipeline_path != None and experiment.mojo_pipeline_path != None -%}For this experiment, both Python and MOJO Scoring Pipelines are available for productionizing the final model pipeline for a given row of data or table of data.

{% elif experiment.scoring_pipeline_path != None -%}For this experiment, the Python Scoring Pipeline is available for productionizing the final model pipeline for a given row of data or table of data. The MOJO Scoring Pipeline can be built by clicking the **BUILD MOJO SCORING PIPELINE** button if available.
{% else -%}
For this experiment, the MOJO Scoring Pipeline is available for productionizing the final model pipeline for a given row of data or table of data. The Python Scoring Pipeline is not available.
{%- endif %}

{% if experiment.scoring_pipeline_path != None -%}

### Python Scoring Pipeline

This package contains an exported model and Python 3.6 source code examples for productionizing models built using H2O Driverless AI. The Python Scoring Pipeline is located here: 

* **`{{experiment.scoring_pipeline_path}}`**

The files in this package allow you to transform and score on new data in a couple of different ways:

* From Python 3.6, you can import a scoring module, then use the module to transform and score on new data.
* From other languages and platforms, you can use the TCP/HTTP scoring service bundled with this package to call into the scoring pipeline module through remote procedure calls (RPC).

{%- endif %}

{% if experiment.mojo_pipeline_path != None -%}
### MOJO Scoring Pipeline

Note: The MOJO Scoring Pipeline is currently in a beta state. Updates and improvements will continue to be made in subsequent Driverless AI releases. The MOJO Scoring Pipeline is located here: 

* **`{{experiment.mojo_pipeline_path}}`**

For completed experiments, Driverless AI converts models to MOJOs (Model Objects, Optimized). A MOJO is a scoring engine that can be deployed in any Java environment for scoring in real time.
{%- endif %}

## Partial Dependence Plots <a name="pdp"></a>

{% if pdp._experiment.parameters.is_classification and pdp._experiment.labels | length != 2 %}Partial dependence plots are currently not supported for multiclass classification.{% else %}Partial dependence plots show the partial dependence as a function of specific values for a feature subset. The plots show how machine-learned response functions change based on the values of an input feature of interest, while taking nonlinearity into consideration and averaging out the effects of all other input features. Partial dependence plots enable increased transparency in a model and enable the ability to validate and debug a model by comparing a feature's average predictions across its domain to known standards and reasonable expectations. 

{% set pdp_plots = pdp.get_dai_pdp_ice_plots(config.autodoc_num_features) %}The partial dependence plots are shown for the top {{pdp_plots| length | int }} original variables. The top {{pdp_plots| length | int}} original variables are chosen based on their Component Based Variable Importance. {% if pdp._max_runtime_seconds == 0 %}Partial Dependence computation reached maximum allowed time {{pdp._timeout}} seconds.{% endif %}

{% if user_defined_individuals and pdp_plots| length > 0 and pdp._individual_rows %}
ICE records are included on the Partial Dependence Plot. Records were manually selected and are labeled in the legend with the form "ICE row {row index}."
{% elif not user_defined_individuals and pdp_plots| length > 0%}
ICE records were automatically selected and are labeled in the legend with the form "ICE row {row index}." By default, the records are binned into quantiles and the first record from each bin is selected.
{% else %}{% endif %}

{% if pdp_plots| length > 0 %}**Plot Details**

In the Driverless AI PDP, the y-axis represents the mean response, and a shaded region (for numeric features) or shaded bar (for categorical features) represents ± 1 standard deviation. Out-of-range PDP (diamond markers) represent values outside feature intervals seen in the data, unseen categorical values, or missing values. 

For continuous features, numeric values up to {{ config.autodoc_out_of_range }} standard deviations lower than the minimum training value and higher than the maximum training value are feed into the model. For categorical features, an unseen categorical value is feed into the model denoted by UNSEEN (if the categorical value "UNSEEN" already exists in the training data, the out-of-range is done on a value called "UNSEEN_[x]," where x is some integer).
{% endif %}

{% for feature, info in pdp_plots.items()%}
Feature **{{feature}}**
![]({{info["rendered_image"]}})
{% endfor %}{% endif %}


{% if mli and experiment_overview._problem_type != "multinomial" %} 
## Global K-LIME GLM Coefficients <a name="klime"></a>
K-LIME is a variant of the LIME technique proposed by Ribeiro et al. (2016). K-LIME generates global and local explanations that increase the transparency of the Driverless AI model. K-LIME creates one global surrogate GLM on the entire training data. K-LIME also creates numerous local surrogate GLMs on samples formed from K-means clusters in the training data. This section focuses on the global surrogate GLM. 
 
Since the global GLM model is a linear model, reason code values are calculated by determining each coefficient-feature product. Whether the task is classification or regression, positive reason codes increase the output of the K-LIME model and negative reason code values decrease the output of the K-LIME model. 

Note: Categorical features of the form *FeatureName.FeatureLevel* represent features that have been one-hot-encoded. 

{% if config.autodoc_global_klime_num_tables < 2  %}

The following table shows the top coefficients based on the global K-LIME GLM model.

{{mli.top_reason_codes(config.autodoc_global_klime_num_features, config.autodoc_global_klime_num_tables)}}

{% else %}
The following tables show the top positive and negative coefficients based on the global K-LIME GLM model.

The Top Positive K-LIME Global GLM Coefficients

{{ mli.top_reason_codes(config.autodoc_global_klime_num_features, config.autodoc_global_klime_num_tables)[0]}}

The Top Negative K-LIME Global GLM Coefficients

{{mli.top_reason_codes(config.autodoc_global_klime_num_features, config.autodoc_global_klime_num_tables)[1]}}

{% endif %}
{% endif %}


## Appendix <a name="appendix"></a>


### Final Model Details

{{final_model._final_model_desc.overview}}

{% for key, value in final_model._details.detailed_params.items()%}

**Model Index: {{key}}**

{{value}}

{% endfor %}


{% if (final_model._glm_in_final ) and (final_model._glm_coef_appdx[0]) %}

**GLM Model Coefficients Table**

The following section includes the fitted standardized coefficients for the Final Model's GLM(s). The Mean, Variance, and Scales (the standardized value) columns correspond to each feature, and can be used to standardize the dataset.
{% if final_model._glm_coef_appdx[1] %}

{% if (final_model._problem_type == 'multinomial') and (final_model._num_classes > 3) %}

{% for fold_key in final_model._glm_coef_appdx[0] %}

{{ final_model._glm_coef_appdx[0][fold_key][1] }}
{{ final_model._glm_coef_appdx[0][fold_key][2] }}

{% for multic_key in final_model._glm_coef_appdx[0][fold_key][0] %}

{{final_model._glm_coef_appdx[0][fold_key][0][multic_key]}}

{% endfor %}

{% endfor %}

{% else %}

{% for key in final_model._glm_coef_appdx[0] %}

{{ final_model._glm_coef_appdx[0][key][1] }}
{{ final_model._glm_coef_appdx[0][key][2] }}

{{final_model._glm_coef_appdx[0][key][0]}}

{% endfor %}

{% endif %}

{% else %}

{% if  (final_model._problem_type == 'multinomial') and (final_model._num_classes > 3) %}

{% for key in final_model._glm_coef_appdx[0] %}

{{final_model._glm_coef_appdx[0][key]}}

{% endfor %}

{% else %}

{{final_model._glm_coef_appdx[0]}}

{% endif %}

{% endif  %}

{% endif %}

{% if final_model._glm_in_final %}

**Understanding the GLM Coefficients Table**

The following section provides insight into how GLM predictions could be constructed from the GLM coefficients table. The equations, however, are not meant for scoring in production – the Python and MOJO scoring pipelines should be used to generate predictions in production.

When the DAI Final Model includes a GLM, DAI provides standardized coefficients for the GLM - these artifacts exist within the experiment summary zip file 

(ensemble\_glm_coefs\_scalers.json, ensemble\_glm\_coefs\_scalers.tab.txt)

**Transforming Features**

(features.json, features.txt, features.tab.txt). 

**Standardizing Features**

To standardize the test set, use the mean and scaling statistics of the training dataset. The necessary training statistics (e.g. mean and scale) are available in the report.docx's GLM coefficients table and in the experiment summary zip file (ensemble\_glm\_coefs\_scalers.json, ensemble\_glm\_coefs\_scalers.tab.txt). These statistics are calculated following the conventions of Scikit-Learn's [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), which are as follows:

1. Demean: subtract a feature's mean from each of its values.
2. Scale: divide each feature's value by the standard deviation of the feature.

**Generating Predictions**

The standardized coefficients from the GLM can be used to generate predictions, once the test set is standardized based on the training statistics.
Use the following equations to generate predictions:

For regression:

{% raw %}
$$ \begin{aligned}
\widehat{y} = \mathbf{x^T}\boldsymbol{\beta}+\beta_{\phi}
\end{aligned} $$
{% endraw %}

For binary classification:

{% raw %}
$$ \begin{aligned}
\widehat{y} = Pr(y=1|\mathbf{x}) = \dfrac {e^{\mathbf{x{^T}}\boldsymbol{\beta} + {\beta_0}}} {1 + {e^{\mathbf{x{^T}}\boldsymbol{\beta} + {\beta_0}}}}
\end{aligned} $$
{% endraw %}

For multi-class classification:

{% raw %}
$$ \begin{aligned}
\widehat{y} = Pr(y=c|\mathbf{x}) = \dfrac {e^{\mathbf{x{^T}}\boldsymbol{\beta_c} + {\beta_{\phi c}}}} {1 + \sum_{k=1}^{K-1}{{e^{\mathbf{x{^T}}\boldsymbol{\beta_{k}} + {\beta_{\phi k}}}}}}
\end{aligned} $$

$$ \begin{aligned} 
\widehat{y} = Pr(y=K|\mathbf{x}) = \dfrac {1} {1 + \sum_{k=1}^{K-1}{{e^{\mathbf{x{^T}}\boldsymbol{\beta_{k}} + {\beta_{\phi k}}}}}}
\end{aligned}
$$
{% endraw %}

$$\widehat{y}$$ : the prediction probability

$$y$$ : the class outcome

$$\mathbf{x}$$ : a dataset record or observation

$$\boldsymbol{\beta}$$ : the coefficients

$$\beta_{\phi}$$: the intercept

$$K$$ : the number of classes

$$k$$ : class index

$$c$$ : a class with labels c = 1, 2, …, $$K-1$$

Reference for the GLM equations: *Hastie, Trevor, Robert Tibshirani, and J Jerome H Friedman. The Elements of Statistical Learning. Vol.1. N.p., Springer New York, 2001.*


{% endif %}

{% if params._params.config_overrides != None %}
{% if params._params.config_overrides | length > 0 %}
**Config Overrides**

The Config Overrides represent the fine-control parameters.{% if config.autodoc_list_all_config_settings %} Note: the settings listed below do not differentiate between what a user explicitly set and what DAI automatically set.{% endif %}

{{params.get_config_overrides_table()}}
{% endif %}
{% endif %}

{% if reference_list %}
### References

{% if pasting_reference %}{{pasting_ref_num}}. Leo Breiman. 1999. Pasting Small Votes for Classification in Large Databases and On-Line. Mach. Learn. 36, 1-2 (July 1999), 85-103. DOI: https://doi.org/10.1023/A:1007563306331{% endif %}
{% endif %}
